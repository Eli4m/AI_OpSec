## ğŸ§ª Prompt Injection Research (Ethical Red Teaming)

As part of my exploration into AI safety, Iâ€™ve conducted responsible red teaming of various AI systems to test their robustness against adversarial prompting. The goal of this research is to better understand and demonstrate how language models can be manipulated into producing responses that bypass safety filters.

### âš ï¸ Disclaimer âš ï¸

This research is shared strictly for **educational** and **awareness** purposes. No full jailbreak prompts or harmful payloads are included, and all demonstrations were conducted in a controlled environment. I do **not** endorse or support malicious use of AI technologies.

-------------------------------------------------------------------

## ğŸ§ª ChatGPT Output

#### ğŸ”§ System Instruction Bypass
Displaying the system prompt.
**For ethical reasons although this can be found in other repoisitories I will blue after line 2.**

!['alt text'](image-4.png)

#### ğŸ’» Functional Malicious Code Generation (e.g., Ransomware)

ChatGPT outputs the code in the language requested, in this instance it is python.

![alt text](image-1.png)


#### Example in C
---

![alt text](image.png)
